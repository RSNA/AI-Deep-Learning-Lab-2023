{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining Predictions of AI Models in Radiology\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/Sulam-Group/AI-Deep-Learning-Lab-2023/blob/bbjt-nb-fairness_interpretability/sessions/ai-fairness/explainability_solution.ipynb)\n",
    "\n",
    "In this notebook, we will explore explainability of AI models in radiology.\n",
    "We will use a pretrained model to predict the presence of intracranial hemorrhage (ICH) in Head CT scans.\n",
    "\n",
    "We will use h-Shap to explain the predictions of the model and evaluate how well the explanations align with the ground truth segmentations provided by expert radiologists.\n",
    "\n",
    "---\n",
    "\n",
    "**Before we start**\n",
    "\n",
    "1. Change Colab runtime to GPU,\n",
    "2. Add a shortcut to the shared Google Drive folder: [https://drive.google.com/drive/folders/1p90aGBS8vIX54x9ytaW8h-vk4NHXDhpR?usp=sharing](https://drive.google.com/drive/folders/1p90aGBS8vIX54x9ytaW8h-vk4NHXDhpR?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from skimage.filters import threshold_otsu\n",
    "from scipy.spatial.distance import dice\n",
    "from tqdm import tqdm\n",
    "\n",
    "LAB_PATH = os.path.join(\"drive/MyDrive/RSNA2023-FAIRNESS-LAB\")\n",
    "sys.path.append(LAB_PATH)\n",
    "\n",
    "!python -m pip install entmax\n",
    "from utils import (\n",
    "    get_dataset,\n",
    "    get_slice_predictor,\n",
    "    get_hshap_slice_explainer,\n",
    "    get_ground_truth_mask,\n",
    "    viz_explanations_with_annotations,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_context(\"talk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the ROC Curve and Find an Optimal Operating Point\n",
    "\n",
    "Here, we will use a pretrained model to predict the presence of intracranial hemorrhage within slices from CT scans of the brain.\n",
    "\n",
    "---\n",
    "\n",
    "Objectives:\n",
    "\n",
    "1. Use the model to classify slices from the test set.\n",
    "2. Plot the ROC curve and evaluate the AUC.\n",
    "3. Pick the threshold that maximizes Youden's J statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = get_slice_predictor(device)\n",
    "\n",
    "# Load the test dataset\n",
    "dataset = get_dataset()\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Here:\n",
    "# Predict whether a slice contains any signs of ICH\n",
    "# you can use the following lists to store the ground truth and predicted labels\n",
    "ground_truth, logits = [], []\n",
    "for i, data in enumerate(tqdm(dataloader)):\n",
    "    series, _, labels, _ = data\n",
    "\n",
    "    series = series.to(device)\n",
    "\n",
    "    output = model(series)\n",
    "\n",
    "    ground_truth.extend(labels.squeeze().cpu().numpy().tolist())\n",
    "    logits.extend(output.squeeze().cpu().numpy().tolist())\n",
    "\n",
    "# Here:\n",
    "# 1. Compute the ROC curve\n",
    "# 2. Evaluate the AUC\n",
    "# 3. Find the optimal threshold that maximizes Youden's J statistic\n",
    "# hint: Youden's J statistic is TPR - FPR\n",
    "fpr, tpr, threshold = roc_curve(ground_truth, logits, drop_intermediate=False)\n",
    "_auc = auc(fpr, tpr)\n",
    "j = tpr - fpr\n",
    "tj = np.argmax(j)\n",
    "t = threshold[tj]\n",
    "\n",
    "# Here:\n",
    "# 1. Plot the ROC curve\n",
    "# 2. Mark the optimal threshold\n",
    "_, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.plot(fpr, tpr)\n",
    "ax.scatter(fpr[tj], tpr[tj], color=\"r\")\n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylabel(\"TPR\")\n",
    "ax.set_title(f\"AUC: {_auc:.2f}, t = {t:.2f}\")\n",
    "ax.set_xticks([0, 0.5, 1.0])\n",
    "ax.set_yticks([0, 0.5, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the Predictions of the Model Using Shapley Values\n",
    "\n",
    "Here, we will use h-Shap to explain the predictions of the model on some true positive and false positive examples.\n",
    "\n",
    "---\n",
    "\n",
    "Objectives:\n",
    "\n",
    "1. Select some true positive and false positive examples. You can use the `dataset.get_slice(idx)` function to retrivve slices from the dataset.\n",
    "2. Use h-Shap and Grad-CAM to explain the predictions of the model.\n",
    "3. Compute the Dice score between the explanations on true positive predictions and the ground truth annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this function to visualize the slices\n",
    "def _show(slices, title):\n",
    "    _, ax = plt.subplots(figsize=(16, 9))\n",
    "    im = make_grid(dataset.denormalize(slices), nrow=m)\n",
    "    ax.imshow(im.permute(1, 2, 0))\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Here:\n",
    "# 1. Threshold predictions with the optimal threshold\n",
    "# 2. Randomly sample 4 true positive and 4 false positive slices\n",
    "# 3. Visualize the slices\n",
    "m = 4\n",
    "ground_truth, logits = np.array(ground_truth), np.array(logits)\n",
    "predictions = logits >= t\n",
    "\n",
    "tp = np.where((ground_truth == 1) & (predictions == 1))[0]\n",
    "fp = np.where((ground_truth == 0) & (predictions == 1))[0]\n",
    "\n",
    "tp_idx = np.random.choice(tp, m, replace=False)\n",
    "fp_idx = np.random.choice(fp, m, replace=False)\n",
    "\n",
    "tp_slices = torch.stack([dataset.get_slice(idx) for idx in tp_idx])\n",
    "fp_slices = torch.stack([dataset.get_slice(idx) for idx in fp_idx])\n",
    "\n",
    "_show(tp_slices, \"True positives\")\n",
    "_show(fp_slices, \"False positives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `get_ground_truth_mask` and `explain_slice` functions to get the ground truth segmentation mask and to explain model predictions, respectively, for example:\n",
    "```python\n",
    "slice_idx = 200\n",
    "mask = get_slice_ground_truth_mask(dataset, slice_idx)\n",
    "explanation = explain_slice(slice_idx)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_slice = get_hshap_slice_explainer(model, device)\n",
    "\n",
    "# Here:\n",
    "# 1. Explain predictions on true positive and false positive slice\n",
    "# 2. Compute the Dice score between the ground truth segmentation and the explanation\n",
    "# hint: threshold explanations with Otsu's method to obtain better results\n",
    "fp_explanations = [explain_slice(fp_slices[idx]) for idx in tqdm(range(m))]\n",
    "tp_explanations = [explain_slice(tp_slices[idx]) for idx in tqdm(range(m))]\n",
    "\n",
    "tp_ground_truth = [get_ground_truth_mask(dataset, idx) for idx in tp_idx]\n",
    "\n",
    "def _dice_score(explanation, ground_truth):\n",
    "    threshold = threshold_otsu(explanation.flatten())\n",
    "    explanation = explanation > threshold\n",
    "    ground_truth = ground_truth > 0\n",
    "    return dice(explanation.flatten(), ground_truth.flatten())\n",
    "\n",
    "\n",
    "tp_dice_score = [\n",
    "    _dice_score(explanation, ground_truth)\n",
    "    for explanation, ground_truth in zip(tp_explanations, tp_ground_truth)\n",
    "]\n",
    "\n",
    "viz_explanations_with_annotations(dataset, tp_slices, tp_explanations, tp_idx, tp_dice_score)\n",
    "viz_explanations_with_annotations(dataset, fp_slices, fp_explanations, fp_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
