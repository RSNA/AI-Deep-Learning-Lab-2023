{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3BhlAw5c/yiv+U16dFDFF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "33ec9d8abc864b12a203cf33842eb6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0bf7e632aac74e5e87616839fc2c874f",
              "IPY_MODEL_2be57b3021bf495a9ef5fe981a035737",
              "IPY_MODEL_d914e327f621465e8e99221ced844849"
            ],
            "layout": "IPY_MODEL_a5b63a301a4f4c55b986f97e0f9664ed"
          }
        },
        "0bf7e632aac74e5e87616839fc2c874f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b22d2920c484cd290a72c0a0e06183d",
            "placeholder": "​",
            "style": "IPY_MODEL_3e353f8fd63e46e7b5d532b5efdceede",
            "value": "llama-2-7b-chat.Q4_K_M.gguf: 100%"
          }
        },
        "2be57b3021bf495a9ef5fe981a035737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8c123570a1c46df87a26f329d35cad0",
            "max": 4081004224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74f630be107e405abb6887b44f90cbd3",
            "value": 4081004224
          }
        },
        "d914e327f621465e8e99221ced844849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f155102b103e421faaf2e0312ba053dc",
            "placeholder": "​",
            "style": "IPY_MODEL_fcf64018fd6c46869a4da0c6e06881de",
            "value": " 4.08G/4.08G [00:20&lt;00:00, 233MB/s]"
          }
        },
        "a5b63a301a4f4c55b986f97e0f9664ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b22d2920c484cd290a72c0a0e06183d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e353f8fd63e46e7b5d532b5efdceede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8c123570a1c46df87a26f329d35cad0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74f630be107e405abb6887b44f90cbd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f155102b103e421faaf2e0312ba053dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcf64018fd6c46869a4da0c6e06881de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RSNA/AI-Deep-Learning-Lab-2023/blob/main/sessions/nlp-text-classification/RSNA23_llama_cpp_report_labeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RSNA 2023: Deep Learning Lab\n",
        "## Report Labeling with Llama.cpp\n",
        "\n",
        "Llama.cpp is a project led by Georgi Gerganov that was initially designed as a pure C/C++ implementation of the Llama large language model developed and open-sourced by Meta's AI team.\n",
        "\n",
        "Quoted from the llama.cpp GitHub repository:\n",
        "\n",
        ">The main goal of llama.cpp is to run the LLaMA model using 4-bit integer quantization on a MacBook\n",
        "> - Plain C/C++ implementation without dependencies\n",
        "> - Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks\n",
        "> - AVX, AVX2 and AVX512 support for x86 architectures\n",
        "> - Mixed F16 / F32 precision\n",
        "> - 2-bit, 3-bit, 4-bit, 5-bit, 6-bit and 8-bit integer quantization support\n",
        "> - CUDA, Metal and OpenCL GPU backend support\n",
        "\n",
        "In lay terms, this means that we can implement these models in such a way that they can be run on nearly any physical or virtual machine! **You don't need an industrial-grade, multi-GPU server to use open-source LLMs locally.**\n",
        "\n",
        "## When to Use an LLM Locally\n",
        "* You have sensitive data that you don't want to send to OpenAI's servers for them to potentially store and use for the training of futures models\n",
        "    - Virtually all healthcare data\n",
        "* You want to fine-tune an open-source LLM for a specific purpose\n",
        "\n",
        "## Overview of This Module\n",
        "1. Install llama.cpp and Hugging Face Hub (to download model files)\n",
        "2. Download the 7 billion parameter Llama2 model fine-tuned for chat\n",
        "3. Engineer a prompt to have the LLM read a chest radiography report and return structured labels for specific findings in JSON format.\n",
        "4. Test a few example reports on Llama2-7B-Chat.\n",
        "5. Repeat the process for the Mistral-7B-Instruct-v0.1 model and compare the results.\n",
        "\n",
        "> _Note: At the time this module was developed, Mistral-7B is the best open-source, 7B parameter model available. This field is moving very quickly, so this very well could change before the end of the year._\n",
        "\n",
        "## References\n",
        "- Llama.cpp on GitHub: https://github.com/ggerganov/llama.cpp\n",
        "- Meta AI's Llama 2: https://ai.meta.com/llama/\n",
        "- MistralAI's Mistral-7B: https://mistral.ai/news/announcing-mistral-7b/\n",
        "- HuggingFace Models:\n",
        "    * [TheBloke/Llama-2-7B-Chat-GGUF](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF)\n",
        "    * [TheBloke/Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)\n",
        "\n",
        "> _Note: If you would like to experiment with other models, please search for the \"GGUF\" version of the model on Hugging Face._"
      ],
      "metadata": {
        "id": "taPtSHHiplPF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cZB8L6RAEZN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Install llama.cpp and HuggingFace Hub\n",
        "# @markdown This cell takes approximately 2 minutes to run. The output is suppressed, so if no error is shown, you may assume that it worked.\n",
        "\n",
        "%%capture\n",
        "\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.11 --force-reinstall --upgrade --no-cache-dir\n",
        "!pip install huggingface_hub==0.18.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Importing the necessary libraries\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "import regex as re\n",
        "import json"
      ],
      "metadata": {
        "id": "pcB7lnFal_5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Select the model you'd like to test\n",
        "\n",
        "# Comment out these two lines and uncomment the two lines at the bottom of the cell to test out Mistral-7B\n",
        "model_name = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n",
        "model_basename = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "\n",
        "# model_name = \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n",
        "# model_basename = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\""
      ],
      "metadata": {
        "id": "sCCZGLHaAr9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download the model from Hugging Face Hub\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name, filename=model_basename)\n",
        "print(model_path)"
      ],
      "metadata": {
        "id": "Y1tHS8FAAp3i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "33ec9d8abc864b12a203cf33842eb6d8",
            "0bf7e632aac74e5e87616839fc2c874f",
            "2be57b3021bf495a9ef5fe981a035737",
            "d914e327f621465e8e99221ced844849",
            "a5b63a301a4f4c55b986f97e0f9664ed",
            "2b22d2920c484cd290a72c0a0e06183d",
            "3e353f8fd63e46e7b5d532b5efdceede",
            "c8c123570a1c46df87a26f329d35cad0",
            "74f630be107e405abb6887b44f90cbd3",
            "f155102b103e421faaf2e0312ba053dc",
            "fcf64018fd6c46869a4da0c6e06881de"
          ]
        },
        "outputId": "23d16bb0-778b-4cd8-b20d-db2e6ae74da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "llama-2-7b-chat.Q4_K_M.gguf:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33ec9d8abc864b12a203cf33842eb6d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/huggingface/hub/models--TheBloke--Llama-2-7b-Chat-GGUF/snapshots/191239b3e26b2882fb562ffccdd1cf0f65402adb/llama-2-7b-chat.Q4_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialize the llama.cpp constructor\n",
        "\n",
        "# Feel free to play around with different hyperparameters below\n",
        "\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU. Should be a power of 2.\n",
        "    n_gpu_layers=36, # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=2048, # Context window = maximum input sequence length (in tokens)\n",
        "    n_gqa=8,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAzp2O8YChWt",
        "outputId": "a697455e-f529-48ce-f160-b7267ac8239e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Engineering\n",
        "\n",
        "Prompt engineering has emerged as an important skill set in getting LLMs to execute your desired task. For this, you should know if there is a **prompt template**\n",
        "\n",
        "1. We start with a `system` prompt. This gives the LLM a role to play in the requests that follow.\n",
        "2. We implement a JSON `schema` to prompt the LLM to return structured labels for each report we submit.\n",
        "3. We provide a sample `report` for the LLM to analyze.\n",
        "4. We construct the `prompt` that will present the report text to the model, ask it to use the JSON schema provided, and analyze the report for the findings included in the schema.\n",
        "5. Finally, we utilize the `prompt templates` for the Llama-2-Chat and Mistral-7B-Instruct-v0.1 models to construct our complete prompt.\n",
        "\n",
        "> _Note: Mistral-7B does not have a separate delimiter for the system role, so we pass that portion of the prompt with the remainder._\n",
        "\n",
        "For more details on prompt engineering, see this guide: [Prompt Engineering Guide](https://www.promptingguide.ai/)"
      ],
      "metadata": {
        "id": "adrff26Swazv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title System prompt\n",
        "\n",
        "system = '''You are an expert radiologist's assistant, skilled in analyzing radiology reports.\n",
        "Please first provide a response to any specific requests. Then explain your reasoning, as appropriate.'''"
      ],
      "metadata": {
        "id": "dTFGbY5PC6Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Construct JSON schema\n",
        "\n",
        "schema = '''\n",
        "{\n",
        "    \"cardiomegaly\": { \"type\": \"boolean\" },\n",
        "    \"lung_opacity\": { \"type\": \"boolean\" },\n",
        "    \"pneumothorax\": { \"type\": \"boolean\" },\n",
        "    \"pleural_effusion\": { \"type\": \"boolean\" },\n",
        "    \"pulmonary_edema\": { \"type\": \"boolean\" },\n",
        "    \"abnormal_study\": { \"type\": \"boolean\" }\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "IqnVrT4dyePY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Provide a sample chest radiograph report\n",
        "\n",
        "report_text = \"No focal consolidation, pneumothorax, or pleural effusion. Cardiomediastinal silhouette is stable and unremarkable. No acute osseous abnormalities are identified. No acute cardiopulmonary abnormality.\""
      ],
      "metadata": {
        "id": "dJ_dGwfTyYv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Construct User prompt\n",
        "\n",
        "prompt = f'''\n",
        "```{report_text}```\n",
        "\n",
        "Please extract the findings from the preceding text radiology report using the following JSON schema:\n",
        "```{schema}```\n",
        "Note that \"lung_opacity\" may include nodule, mass, atelectasis, or consolidation.\n",
        "'''"
      ],
      "metadata": {
        "id": "A4BM5fKTyV0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Llama-2-Chat & Mistral-7B-Instruct-v0.1 prompt templates\n",
        "\n",
        "llama2_prompt_template = f'''[INST] <<SYS>>\n",
        "{system}\n",
        "<</SYS>>\n",
        "{prompt}[/INST]\n",
        "'''\n",
        "\n",
        "mistral_prompt_template = f'''<s>[INST] {system} {prompt} [/INST]'''"
      ],
      "metadata": {
        "id": "NIAMoEGlyPXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate LLM response and print response text\n",
        "\n",
        "response = lcpp_llm(\n",
        "    prompt=llama2_prompt_template, # Comment out this line and uncomment the line below to test Mistral-7B\n",
        "    # prompt=mistral_prompt_template,\n",
        "    max_tokens=512,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    top_k=50,\n",
        "    # echo=True, # return the prompt\n",
        ");\n",
        "\n",
        "res_txt = response[\"choices\"][0][\"text\"]\n",
        "print(res_txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmk1Vae7iowg",
        "outputId": "b30002a5-4688-4893-f292-36a73b37d084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course! I'd be happy to help you extract the findings from the radiology report using the provided JSON schema. Here are the results of my analysis:\n",
            "{\n",
            "    \"cardiomegaly\": false,\n",
            "    \"lung_opacity\": {\n",
            "        \"type\": \"boolean\",\n",
            "        \"value\": true\n",
            "    },\n",
            "    \"pneumothorax\": false,\n",
            "    \"pleural_effusion\": false,\n",
            "    \"pulmonary_emia\": false,\n",
            "    \"abnormal_study\": true\n",
            "}\n",
            "Explanation:\n",
            "* The report states that there is no focal consolidation, pneumothorax, or pleural effusion. Therefore, the values for these fields in the JSON schema are set to false.\n",
            "* The report does mention that the cardiomediastinal silhouette is stable and unremarkable, which means that the field \"cardiomegaly\" should be set to false as well.\n",
            "* Under the \"lung_opacity\" field, the report mentions that there is opacity in both lungs, which could indicate nodules, masses, atelectasis, or consolidation. Therefore, the value for this field is set to true.\n",
            "* The report does not mention any acute osseous abnormalities or cardiopulmonary abnormalities, so these fields should be left blank in the JSON schema.\n",
            "I hope this helps! Let me know if you have any further questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations of this Approach\n",
        "\n",
        "1. **Errors:** You may notice when using Llama-2-7B-Chat or other models that the JSON returned is not ideal for what we requested or may even have an error like turning `pulmonary_edema` into `pulmonary_emia`.\n",
        "    - This can be improved by simplifying your request for smaller models or using a model that is better trained for returning structured data in JSON format, like Mistral-7B.\n",
        "    - Playing around with some of the model inference hyperparameters can also help. See this guide for further details: [Prompt Engineering Guide: LLM Settings](https://www.promptingguide.ai/introduction/settings)\n",
        "2. **Hallucinations:** LLMs can provide very confident answers that are flat out wrong. You may see output like `\"Under the 'lung_opacity' field, the report mentions that there is opacity in both lungs, which could indicate nodules, masses, atelectasis, or consolidation. Therefore, the value for this field is set to true.\"`, even when there is no mention of that in the report referenced!\n",
        "    - This can be improved by careful prompt engineering. You may want to include in your `system prompt` an instruction to not return an answer if the model is not confident. Or you may want to try without having the model explain it's reasoning.\n",
        "    - A group at NIH found that asking Vicuna-13B to perform a single labeling task at one time provided more robust results in this article published in _Radiology_: [Feasibility of Using the Privacy-preserving Large Language Model Vicuna for Labeling Radiology Reports](https://pubs.rsna.org/doi/10.1148/radiol.231147)\n",
        "    - For certain use cases, retrieval-augmented generation (RAG) can be helpful. We'll cover that in the next notebook.\n",
        "    - Finally, if all else fails and you have several hundred labeled examples of the task you want the LLM to perform, you may consider parameter-efficient fine-tuning (PEFT). See this guide from NVIDIA for more details: [Selecting LLM Customization Techniques](https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/)"
      ],
      "metadata": {
        "id": "oq7P_I9o7_vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define a function to postprocess the response text and extract the JSON object into a Python dict\n",
        "\n",
        "def json_from_str(s):\n",
        "    expr = re.compile(r'\\{(?:[^{}]*|(?R))*\\}')\n",
        "    res = expr.findall(s)\n",
        "    return json.loads(res[0]) if res else None\n"
      ],
      "metadata": {
        "id": "oRLe1laSi_22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Assign an ID number to the report and associate extracted labels with the report ID\n",
        "\n",
        "id = 1\n",
        "labels = json_from_str(res_txt)\n",
        "result_dict = {id: labels}\n",
        "result_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKiAdU2Hq4s0",
        "outputId": "09e5cf4a-b265-4482-d189-a69ee32135aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: {'cardiomegaly': False,\n",
              "  'lung_opacity': {'type': 'boolean', 'value': True},\n",
              "  'pneumothorax': False,\n",
              "  'pleural_effusion': False,\n",
              "  'pulmonary_emia': False,\n",
              "  'abnormal_study': True}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bJ-jDxOcyggT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}